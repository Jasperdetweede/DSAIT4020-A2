{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 381,
   "id": "3b0f453c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 382,
   "id": "6f4038d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Steps for preprocessing:\n",
    "# 1. Drop columns with more than 50% missing values\n",
    "# 2. Replace NHANES special codes (7/77/777, 9/99/999) with NaN\n",
    "# 3. Binary variables: convert to binary Yes/No (1/2) to (1/0)\n",
    "#    For example, DBQ930 - Main meal planner/preparer (1=Yes, 2=No, 7=Refused, 9=Don't know)\n",
    "# 4. Categorical variables:\n",
    "#    - Ordinal variables, e.g. FNQ410 - Food security status: 1=Full food security, 2=Marginal food security, 3=Low food security, 4=Very low food security\n",
    "#       - Preserve ordinality\n",
    "#       - Convert to 0-indexed (0-3)\n",
    "# 5. Continous variables:\n",
    "#    - Frequency variables, e.g. ALQ121 - Alcohol frequency past 12 months), 0=Never in the past year, 1=Every day, 2=5-6 days/week,...\n",
    "#      Convert to days per year for continuous analysis\n",
    "        # alq121_mapping = {\n",
    "        #     0: 0,      # Never\n",
    "        #     1: 365,    # Every day\n",
    "        #     2: 286,    # 5-6 days/week → ~5.5*52\n",
    "        #     3: 182,    # 3-4 days/week → ~3.5*52\n",
    "        #     4: 104,    # 2 days/week\n",
    "        #     5: 78,     # 1 day/week\n",
    "        #     6: 52,     # 2-3 days/month\n",
    "        #     7: 36,     # 1 day/month\n",
    "        #     8: 24,     # 7-11 times/year\n",
    "        #     9: 15,     # 3-6 times/year\n",
    "        #     10: 6      # 1-2 times/year\n",
    "        # }\n",
    "        # df['ALQ121_days'] = df['ALQ121'].map(alq121_mapping)\n",
    "#   - Numerical measurements (WHD010=height, WHD020=weight, WHD050=desired weight): \n",
    "#     - Check for outliers and reasonable ranges\n",
    "#     - Standardize units if necessary\n",
    "#     - Create new features, e.g., BMI from height and weight\n",
    "# 6. Target variable: DSM-V based depression diagnosis\n",
    "\n",
    "# Missing values?\n",
    "# Combining features?\n",
    "# Encode categorical variables?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b308b972",
   "metadata": {},
   "source": [
    "### Load dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 383,
   "id": "e41e9da2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Numerical columns: ['SEQN', 'DPQ010', 'DPQ020', 'DPQ030', 'DPQ040', 'DPQ050', 'DPQ060', 'DPQ070', 'DPQ080', 'DPQ090', 'DPQ100', 'ACD010A', 'ACD010B', 'ACD010C', 'ACD040', 'ALQ111', 'ALQ121', 'ALQ130', 'ALQ142', 'ALQ270', 'ALQ280', 'ALQ151', 'ALQ170', 'BPQ020', 'BPQ030', 'BPQ150', 'BPQ080', 'BPQ101D', 'DBQ010', 'DBD030', 'DBD041', 'DBD050', 'DBD055', 'DBD061', 'DBQ073A', 'DBQ073B', 'DBQ073C', 'DBQ073D', 'DBQ073E', 'DBQ073U', 'DBQ301', 'DBQ330', 'DBQ360', 'DBQ370', 'DBD381', 'DBQ390', 'DBQ400', 'DBD411', 'DBQ421', 'DBQ424', 'DBQ930', 'DBQ935', 'DBQ940', 'DBQ945', 'DIQ010', 'DID040', 'DIQ160', 'DIQ180', 'DIQ050', 'DID060', 'DIQ060U', 'DIQ070', 'FNQ021', 'FNQ041', 'FNQ050', 'FNQ060', 'FNQ080', 'FNQ160', 'FNQ100', 'FNQ110', 'FNQ120', 'FNQ170', 'FNQ180', 'FNQ190', 'FNQ130', 'FNQ200', 'FNQ140', 'FNQ150', 'FNDCDI', 'FNQ410', 'FNQ430', 'FNQ440', 'FNQ450', 'FNQ460', 'FNQ470', 'FNQ480', 'FNQ490', 'FNQ510', 'FNQ520', 'FNQ530', 'FNQ540', 'FNDADI', 'FNDAEDI', 'FSD032A', 'FSD032B', 'FSD032C', 'FSD041', 'FSD052', 'FSD061', 'FSD071', 'FSD081', 'FSD092', 'FSD102', 'FSDAD', 'FSD151', 'FSQ165', 'FSD165N', 'FSQ012', 'FSD012N', 'FSD230', 'FSD230N', 'FSD795', 'FSD225', 'FSD235', 'FSD162', 'FSQ760', 'FSD760N', 'FSQ653', 'FSD660ZC', 'FSD675', 'FSD680', 'FSD670ZC', 'FSQ690', 'FSQ695', 'HIQ011', 'HIQ032A', 'HIQ032B', 'HIQ032C', 'HIQ032D', 'HIQ032E', 'HIQ032F', 'HIQ032H', 'HIQ032I', 'HIQ210', 'HOD051', 'HSQ590', 'HUQ010', 'HUQ030', 'HUQ042', 'HUQ055', 'HUQ090', 'INDFMMPI', 'INDFMMPC', 'INQ300', 'IND310', 'KIQ022', 'KIQ025', 'KIQ005', 'KIQ010', 'KIQ042', 'KIQ044', 'KIQ052', 'KIQ481', 'MCQ010', 'MCQ035', 'MCQ040', 'MCQ050', 'AGQ030', 'MCQ053', 'MCQ149', 'MCQ160A', 'MCQ195', 'MCQ160B', 'MCQ160C', 'MCQ160D', 'MCQ160E', 'MCQ160F', 'MCQ160M', 'MCQ170M', 'MCQ160P', 'MCQ160L', 'MCQ170L', 'MCQ500', 'MCQ510A', 'MCQ510B', 'MCQ510C', 'MCQ510D', 'MCQ510E', 'MCQ510F', 'MCQ550', 'MCQ560', 'MCQ220', 'MCQ230A', 'MCQ230B', 'MCQ230C', 'MCQ230D', 'OSQ230', 'OCD150', 'OCQ180', 'OCQ210', 'OCQ215', 'OCQ383', 'OHQ845', 'OHQ620', 'OHQ630', 'OHQ640', 'OHQ660', 'OHQ670', 'OHQ680', 'PAQ706', 'PAQ711', 'PAD790Q', 'PAD800', 'PAD810Q', 'PAD820', 'PAD680', 'RHQ010', 'RHQ031', 'RHD043', 'RHQ060', 'RHQ078', 'RHQ131', 'RHD143', 'RHD167', 'RHQ200', 'RHD280', 'RHQ305', 'RHQ332', 'RXQ510', 'RXQ515', 'RXQ520', 'RXQ033', 'RXQ050', 'SLD012', 'SLD013', 'SMD460', 'SMD470', 'SMQ681', 'SMQ690A', 'SMQ710', 'SMQ720', 'SMQ725', 'SMQ690B', 'SMQ740', 'SMQ690C', 'SMQ770', 'SMQ690G', 'SMQ845', 'SMQ846', 'SMQ849', 'SMQ851', 'SMQ690D', 'SMQ690E', 'SMQ690K', 'SMQ863', 'SMQ690F', 'SMDANY', 'SMQ020', 'SMQ040', 'SMD641', 'SMD650', 'SMD100MN', 'SMQ621', 'SMD630', 'SMAQUEX2', 'WHD010', 'WHD020', 'WHD050', 'WHQ070']\n",
      "Categorical columns: ['PAD790U', 'PAD810U', 'SLQ300', 'SLQ310', 'SLQ320', 'SLQ330']\n"
     ]
    }
   ],
   "source": [
    "df_depression_data = pd.read_csv('processed_data/depression_data.csv')\n",
    "numerical_cols = df_depression_data.select_dtypes(include=['number', 'float64', 'int64', 'datetime']).columns.tolist()\n",
    "categorical_cols = df_depression_data.select_dtypes(include=['category', 'object']).columns.tolist()\n",
    "\n",
    "print(f\"Numerical columns: {numerical_cols}\")\n",
    "print(f\"Categorical columns: {categorical_cols}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17900cee",
   "metadata": {},
   "source": [
    "### Handle columns with missing values\n",
    "Replace NHANES codes for refuse/missing wih NaN and drop columns with >45% missing values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 384,
   "id": "46efa6de",
   "metadata": {},
   "outputs": [],
   "source": [
    "def replace_nhanes_special_codes(df):\n",
    "    \"\"\"\n",
    "    Replace NHANES special codes with NaN.\n",
    "    - 7, 77, 777, 7777, 77777 = Refused\n",
    "    - 9, 99, 999, 9999, 99999, 55555 = Don't know / Missing\n",
    "    \"\"\"\n",
    "    # Define special codes to replace\n",
    "    refused_codes = [7, 77, 777, 7777, 77777]\n",
    "    dont_know_codes = [9, 99, 999, 9999, 99999, 55555]\n",
    "    special_codes = refused_codes + dont_know_codes\n",
    "    \n",
    "    # Also handle implausible values in specific columns\n",
    "    implausible_values = {\n",
    "        'WHD010': [9999],      # Height\n",
    "        'WHD020': [9999],      # Weight  \n",
    "        'WHD050': [9999, 7777], # Desired weight\n",
    "        'PAD680': [9999],      # Sedentary minutes\n",
    "        'OCQ180': [99999, 77777], # Work hours\n",
    "        'DID040': [999],       # Age diabetes diagnosed\n",
    "        'RHQ332': [999],       # Age ovaries removed\n",
    "    }\n",
    "    \n",
    "    replaced_count = 0\n",
    "    for col in df.select_dtypes(include=[np.number]).columns:\n",
    "        # Replace standard special codes\n",
    "        mask = df[col].isin(special_codes)\n",
    "        replaced_count += mask.sum()\n",
    "        df.loc[mask, col] = np.nan\n",
    "        \n",
    "        # Replace column-specific implausible values\n",
    "        if col in implausible_values:\n",
    "            mask = df[col].isin(implausible_values[col])\n",
    "            replaced_count += mask.sum()\n",
    "            df.loc[mask, col] = np.nan\n",
    "    \n",
    "    print(f\"Replaced {replaced_count} special code values with NaN\")\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 385,
   "id": "ebf75fa3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def drop_high_missing_columns(df, threshold=50):\n",
    "    \"\"\"Drop columns with missing percentage above threshold.\"\"\"\n",
    "    missing_pct = df.isna().sum() / len(df) * 100\n",
    "    cols_to_drop = missing_pct[missing_pct > threshold].index.tolist()\n",
    "    df = df.drop(columns=cols_to_drop)\n",
    "    print(f\"Dropped {len(cols_to_drop)} columns with >{threshold}% missing values\")\n",
    "    print(f\"Shape after dropping high-missing columns: {df.shape}\")\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 386,
   "id": "7348cd98",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Replaced 8750 special code values with NaN\n",
      "Dropped 146 columns with >45% missing values\n",
      "Shape after dropping high-missing columns: (4167, 119)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>DPQ010</th>\n",
       "      <th>DPQ020</th>\n",
       "      <th>DPQ030</th>\n",
       "      <th>DPQ040</th>\n",
       "      <th>DPQ050</th>\n",
       "      <th>DPQ060</th>\n",
       "      <th>DPQ070</th>\n",
       "      <th>DPQ080</th>\n",
       "      <th>DPQ090</th>\n",
       "      <th>DPQ100</th>\n",
       "      <th>...</th>\n",
       "      <th>SMQ681</th>\n",
       "      <th>SMQ846</th>\n",
       "      <th>SMQ851</th>\n",
       "      <th>SMQ863</th>\n",
       "      <th>SMDANY</th>\n",
       "      <th>SMQ020</th>\n",
       "      <th>WHD010</th>\n",
       "      <th>WHD020</th>\n",
       "      <th>WHD050</th>\n",
       "      <th>WHQ070</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>70.0</td>\n",
       "      <td>220.0</td>\n",
       "      <td>220.0</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>60.0</td>\n",
       "      <td>150.0</td>\n",
       "      <td>165.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>68.0</td>\n",
       "      <td>200.0</td>\n",
       "      <td>180.0</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>69.0</td>\n",
       "      <td>220.0</td>\n",
       "      <td>265.0</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>61.0</td>\n",
       "      <td>228.0</td>\n",
       "      <td>235.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4162</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>67.0</td>\n",
       "      <td>204.0</td>\n",
       "      <td>234.0</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4163</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>67.0</td>\n",
       "      <td>245.0</td>\n",
       "      <td>290.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4164</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>62.0</td>\n",
       "      <td>169.0</td>\n",
       "      <td>172.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4165</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>65.0</td>\n",
       "      <td>180.0</td>\n",
       "      <td>180.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4166</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>64.0</td>\n",
       "      <td>157.0</td>\n",
       "      <td>157.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>4167 rows × 115 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      DPQ010  DPQ020  DPQ030  DPQ040  DPQ050  DPQ060  DPQ070  DPQ080  DPQ090  \\\n",
       "0        0.0     0.0     1.0     0.0     0.0     0.0     0.0     0.0     0.0   \n",
       "1        0.0     0.0     1.0     1.0     0.0     0.0     0.0     0.0     0.0   \n",
       "2        0.0     0.0     0.0     0.0     0.0     1.0     0.0     0.0     0.0   \n",
       "3        3.0     3.0     3.0     3.0     3.0     3.0     3.0     2.0     1.0   \n",
       "4        0.0     0.0     0.0     1.0     0.0     0.0     0.0     0.0     0.0   \n",
       "...      ...     ...     ...     ...     ...     ...     ...     ...     ...   \n",
       "4162     0.0     0.0     1.0     1.0     0.0     0.0     0.0     0.0     0.0   \n",
       "4163     0.0     0.0     0.0     0.0     1.0     0.0     0.0     0.0     0.0   \n",
       "4164     1.0     0.0     1.0     1.0     0.0     0.0     0.0     0.0     0.0   \n",
       "4165     1.0     1.0     0.0     1.0     1.0     0.0     1.0     0.0     0.0   \n",
       "4166     0.0     0.0     0.0     1.0     0.0     0.0     0.0     0.0     0.0   \n",
       "\n",
       "      DPQ100  ...  SMQ681  SMQ846  SMQ851  SMQ863  SMDANY  SMQ020  WHD010  \\\n",
       "0        0.0  ...     2.0     2.0     2.0     2.0     2.0     1.0    70.0   \n",
       "1        0.0  ...     2.0     2.0     2.0     2.0     2.0     2.0    60.0   \n",
       "2        0.0  ...     2.0     2.0     2.0     2.0     2.0     1.0    68.0   \n",
       "3        2.0  ...     1.0     1.0     2.0     2.0     1.0     1.0    69.0   \n",
       "4        0.0  ...     2.0     2.0     2.0     2.0     2.0     2.0    61.0   \n",
       "...      ...  ...     ...     ...     ...     ...     ...     ...     ...   \n",
       "4162     0.0  ...     2.0     2.0     2.0     2.0     2.0     2.0    67.0   \n",
       "4163     0.0  ...     2.0     2.0     2.0     2.0     2.0     1.0    67.0   \n",
       "4164     0.0  ...     2.0     2.0     2.0     2.0     2.0     1.0    62.0   \n",
       "4165     1.0  ...     2.0     2.0     2.0     2.0     2.0     1.0    65.0   \n",
       "4166     0.0  ...     2.0     2.0     2.0     2.0     2.0     1.0    64.0   \n",
       "\n",
       "      WHD020  WHD050  WHQ070  \n",
       "0      220.0   220.0     2.0  \n",
       "1      150.0   165.0     1.0  \n",
       "2      200.0   180.0     2.0  \n",
       "3      220.0   265.0     2.0  \n",
       "4      228.0   235.0     1.0  \n",
       "...      ...     ...     ...  \n",
       "4162   204.0   234.0     2.0  \n",
       "4163   245.0   290.0     1.0  \n",
       "4164   169.0   172.0     1.0  \n",
       "4165   180.0   180.0     1.0  \n",
       "4166   157.0   157.0     1.0  \n",
       "\n",
       "[4167 rows x 115 columns]"
      ]
     },
     "execution_count": 386,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# replace NHANES special codes with NaN\n",
    "df_depression_data = replace_nhanes_special_codes(df_depression_data)\n",
    "# drop columns with more than 50% missing values\n",
    "df_depression_data = drop_high_missing_columns(df_depression_data, threshold=45)\n",
    "# drop id column, unnecessary flag column and target leakage columns\n",
    "df_depression_data.drop(columns=['SEQN', 'SMAQUEX2', 'FNQ530', 'FNQ540'], inplace=True, errors='ignore')\n",
    "df_depression_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 387,
   "id": "ea2415e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Numerical columns: ['DPQ010', 'DPQ020', 'DPQ030', 'DPQ040', 'DPQ050', 'DPQ060', 'DPQ070', 'DPQ080', 'DPQ090', 'DPQ100', 'ACD010A', 'ALQ111', 'ALQ121', 'ALQ130', 'ALQ142', 'ALQ151', 'BPQ020', 'BPQ080', 'BPQ101D', 'DBQ930', 'DBQ935', 'DBQ940', 'DBQ945', 'DIQ010', 'DIQ160', 'DIQ180', 'FNQ410', 'FNQ430', 'FNQ440', 'FNQ450', 'FNQ460', 'FNQ470', 'FNQ480', 'FNQ490', 'FNQ510', 'FNQ520', 'FNDADI', 'FNDAEDI', 'FSD032A', 'FSD032B', 'FSD032C', 'FSDAD', 'FSD151', 'FSQ165', 'FSD162', 'HIQ011', 'HIQ210', 'HOD051', 'HSQ590', 'HUQ010', 'HUQ030', 'HUQ042', 'HUQ055', 'HUQ090', 'INDFMMPI', 'INDFMMPC', 'INQ300', 'KIQ022', 'KIQ005', 'KIQ042', 'KIQ044', 'KIQ481', 'MCQ010', 'AGQ030', 'MCQ053', 'MCQ160A', 'MCQ160B', 'MCQ160C', 'MCQ160D', 'MCQ160E', 'MCQ160F', 'MCQ160M', 'MCQ160P', 'MCQ160L', 'MCQ550', 'MCQ560', 'MCQ220', 'OSQ230', 'OCD150', 'OHQ845', 'OHQ620', 'OHQ630', 'OHQ640', 'OHQ660', 'OHQ670', 'OHQ680', 'PAD790Q', 'PAD800', 'PAD810Q', 'PAD680', 'RHQ010', 'RHQ031', 'RHD280', 'RXQ510', 'RXQ033', 'RXQ050', 'SLD012', 'SLD013', 'SMD460', 'SMQ681', 'SMQ846', 'SMQ851', 'SMQ863', 'SMDANY', 'SMQ020', 'WHD010', 'WHD020', 'WHD050', 'WHQ070']\n",
      "Categorical columns: ['PAD790U', 'PAD810U', 'SLQ300', 'SLQ310', 'SLQ320', 'SLQ330']\n"
     ]
    }
   ],
   "source": [
    "numerical_cols = df_depression_data.select_dtypes(include=['number', 'float64', 'int64', 'datetime']).columns.tolist()\n",
    "categorical_cols = df_depression_data.select_dtypes(include=['category', 'object']).columns.tolist()\n",
    "\n",
    "print(f\"Numerical columns: {numerical_cols}\")\n",
    "print(f\"Categorical columns: {categorical_cols}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4627b396",
   "metadata": {},
   "source": [
    "### Identify categorical and numerical columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 388,
   "id": "b440cb2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def identify_column_types(df, target_cols):\n",
    "    \"\"\"\n",
    "    Separate columns into ordinal, nominal, binary, object (categorical) and numerical types.\n",
    "    \"\"\"\n",
    "    exclude_cols = set(target_cols)\n",
    "    object_cols = df.select_dtypes(include='object').columns.tolist()\n",
    "    exclude_cols.update(object_cols)\n",
    "\n",
    "    # Known ordinal and nominal columns based on NHANES codebooks\n",
    "    ordinal_cols = [\n",
    "        'HUQ010',\n",
    "        'FNQ410', 'FNQ430', 'FNQ440', 'FNQ450', 'FNQ460', \n",
    "        'FNQ470', 'FNQ480', 'FNQ490',\n",
    "        'FNQ510', 'FNQ520',\n",
    "        'FSD032A', 'FSD032B', 'FSD032C', 'FSDAD',\n",
    "        'ALQ121', 'ALQ142',\n",
    "        'OHQ845', 'OHQ620', 'OHQ630', 'OHQ640', 'OHQ660', 'OHQ670', 'OHQ680',\n",
    "        'DIQ010', 'INDFMMPC', 'KIQ005', 'SMD460',\n",
    "    ]\n",
    "    \n",
    "    nominal_cols = [\n",
    "        'HUQ042', 'OCD150', 'HUQ030',\n",
    "    ]\n",
    "\n",
    "    # Identify binary columns (Yes=1, No=2)\n",
    "    binary_cols = []\n",
    "    for col in df.select_dtypes(include=[np.number]).columns:\n",
    "        unique_vals = set(df[col].dropna().unique())\n",
    "        if col not in exclude_cols and unique_vals and unique_vals.issubset({1, 2, 1.0, 2.0}):\n",
    "            binary_cols.append(col)\n",
    "\n",
    "    known_categorical = set(ordinal_cols + nominal_cols + binary_cols)\n",
    "\n",
    "    # Build numerical list\n",
    "    numerical_cols = []\n",
    "    for col in df.select_dtypes(include=[np.number]).columns:\n",
    "        if col not in known_categorical and col not in exclude_cols:\n",
    "            numerical_cols.append(col)\n",
    "    \n",
    "    return ordinal_cols, nominal_cols, binary_cols, numerical_cols, object_cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 389,
   "id": "7bc3f04b",
   "metadata": {},
   "outputs": [],
   "source": [
    "(ordinal_cols, nominal_cols, \n",
    " binary_cols, numerical_cols, \n",
    " object_cols) = identify_column_types(\n",
    "    df_depression_data,\n",
    "    target_cols=['DPQ010', 'DPQ020', 'DPQ030', 'DPQ040', 'DPQ050', 'DPQ060', 'DPQ070', 'DPQ080', 'DPQ090', 'DPQ100'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 390,
   "id": "af26efb1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ordinal columns: 28\n",
      "Nominal columns: 3\n",
      "Binary columns: 53\n",
      "Numerical columns: 15\n",
      "Object columns (excluded): 6\n",
      "Total columns identified: 105\n"
     ]
    }
   ],
   "source": [
    "print(f\"Ordinal columns: {len(ordinal_cols)}\")\n",
    "print(f\"Nominal columns: {len(nominal_cols)}\")\n",
    "print(f\"Binary columns: {len(binary_cols)}\")\n",
    "print(f\"Numerical columns: {len(numerical_cols)}\")\n",
    "print(f\"Object columns (excluded): {len(object_cols)}\")\n",
    "print(f\"Total columns identified: {len(ordinal_cols) + len(nominal_cols) + len(binary_cols) + len(numerical_cols) + len(object_cols)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 391,
   "id": "15323121",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique values in PAD790U: [\"b'W'\" \"b'D'\" \"b''\" \"b'Y'\" \"b'M'\"]\n",
      "Unique values in PAD810U: [\"b'W'\" \"b''\" \"b'M'\" \"b'D'\" \"b'Y'\"]\n",
      "Unique values in SLQ300: [\"b'21:00'\" \"b'00:00'\" \"b'03:00'\" \"b'22:30'\" \"b'23:30'\" \"b'22:00'\"\n",
      " \"b'02:00'\" \"b'23:00'\" \"b''\" \"b'07:30'\"]\n",
      "Unique values in SLQ310: [\"b'06:00'\" \"b'08:00'\" \"b'07:30'\" \"b'10:30'\" \"b'06:30'\" \"b'10:00'\"\n",
      " \"b'07:00'\" \"b'05:30'\" \"b''\" \"b'14:30'\"]\n",
      "Unique values in SLQ320: [\"b'21:00'\" \"b'00:00'\" \"b'03:00'\" \"b'22:30'\" \"b'00:30'\" \"b'01:00'\"\n",
      " \"b'22:00'\" \"b'02:00'\" \"b'23:00'\" \"b'20:30'\"]\n",
      "Unique values in SLQ330: [\"b'06:00'\" \"b'09:00'\" \"b'08:00'\" \"b'10:30'\" \"b'07:30'\" \"b'07:00'\"\n",
      " \"b'10:00'\" \"b'11:00'\" \"b'08:30'\" \"b'09:30'\"]\n"
     ]
    }
   ],
   "source": [
    "for col in object_cols:\n",
    "    print(f\"Unique values in {col}: {df_depression_data[col].unique()[:10]}\")  # Print first 10 unique values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4858ea40",
   "metadata": {},
   "source": [
    "### Handle object columns\n",
    "Transform times into decimal hours and add them to numerical columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 392,
   "id": "cd28b477",
   "metadata": {},
   "outputs": [],
   "source": [
    "def time_to_hours(time_str):\n",
    "    \"\"\"Convert 'HH:MM' to decimal hours (0-24)\"\"\"\n",
    "    if pd.isna(time_str):\n",
    "        return np.nan\n",
    "    try:\n",
    "        h, m = time_str.split(':')\n",
    "        return int(h) + int(m) / 60\n",
    "    except:\n",
    "        return np.nan\n",
    "\n",
    "df_depression_data['SLQ300'] = df_depression_data['SLQ300'].apply(time_to_hours)\n",
    "df_depression_data['SLQ310'] = df_depression_data['SLQ310'].apply(time_to_hours)\n",
    "df_depression_data['SLQ320'] = df_depression_data['SLQ320'].apply(time_to_hours)\n",
    "df_depression_data['SLQ330'] = df_depression_data['SLQ330'].apply(time_to_hours)\n",
    "\n",
    "numerical_cols.extend(['SLQ300', 'SLQ310', 'SLQ320', 'SLQ330'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54b601fd",
   "metadata": {},
   "source": [
    "### Special ordinal cases\n",
    "- Most ordinal features have 1: Yes, 2: No. We need to convert 2 to 0 for proper encoding\n",
    "- Some features (e.g., DIQ010) are ordinal but unordered: 1: Yes, 2: No, 3: Borderline\n",
    "- Some features are in reverse order (1: every day, 2: nearly every day, etc.), when it should be higher number = higher frequency/severity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 393,
   "id": "03667e3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert 2 to 0 for binary columns\n",
    "for col in binary_cols:\n",
    "    df_depression_data[col] = df_depression_data[col].replace({2: 0, 2.0: 0})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 394,
   "id": "ed865b9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for some ordinal features, higher value = less frequent/severe => should be reversed\n",
    "reverse_ordinal = {\n",
    "    # FNQ510: 1=daily → 5=never (should be: higher = more frequent)\n",
    "    'FNQ510': {1: 4, 2: 3, 3: 2, 4: 1, 5: 0},  # daily→4, weekly→3, monthly→2, few times→1, never→0\n",
    "    \n",
    "    # OHQ620-OHQ680: 1=very often → 5=never (should be: higher = more problems)\n",
    "    'OHQ620': {1: 4, 2: 3, 3: 2, 4: 1, 5: 0},\n",
    "    'OHQ630': {1: 4, 2: 3, 3: 2, 4: 1, 5: 0},\n",
    "    'OHQ640': {1: 4, 2: 3, 3: 2, 4: 1, 5: 0},\n",
    "    'OHQ660': {1: 4, 2: 3, 3: 2, 4: 1, 5: 0},\n",
    "    'OHQ670': {1: 4, 2: 3, 3: 2, 4: 1, 5: 0},\n",
    "    'OHQ680': {1: 4, 2: 3, 3: 2, 4: 1, 5: 0},\n",
    "    \n",
    "    # FSD032A-C: 1=often true → 3=never true (should be: higher = more food insecurity)\n",
    "    'FSD032A': {1: 2, 2: 1, 3: 0},\n",
    "    'FSD032B': {1: 2, 2: 1, 3: 0},\n",
    "    'FSD032C': {1: 2, 2: 1, 3: 0},\n",
    "\n",
    "    # ALQ121, ALQ142: 0=Never, 1=Every day → 10=1-2 times a year (should be: higher = more frequent)\n",
    "    'ALQ121': {0: 0, 10: 1, 9: 2, 8: 3, 7: 4, 6: 5, 5: 6, 4: 7, 3: 8, 2: 9, 1: 10},\n",
    "    'ALQ142': {0: 0, 10: 1, 9: 2, 8: 3, 7: 4, 6: 5, 5: 6, 4: 7, 3: 8, 2: 9, 1: 10},\n",
    "}\n",
    "\n",
    "for col, mapping in reverse_ordinal.items():\n",
    "    df_depression_data[col] = df_depression_data[col].map(mapping)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 395,
   "id": "0b92e4c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DIQ010: 1=Yes, 2=No, 3=Borderline\n",
    "# Correct order: No(0) → Borderline(1) → Yes(2)\n",
    "diq010_remap = {2: 0, 3: 1, 1: 2}\n",
    "\n",
    "# FNQ520: 1=a little, 2=a lot, 3=somewhere in between\n",
    "# Correct order: little(0) → between(1) → a lot(2)\n",
    "fnq520_remap = {1: 0, 3: 1, 2: 2}\n",
    "\n",
    "df_depression_data['DIQ010'] = df_depression_data['DIQ010'].map(diq010_remap)\n",
    "df_depression_data['FNQ520'] = df_depression_data['FNQ520'].map(fnq520_remap)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9227a5c3",
   "metadata": {},
   "source": [
    "### Shift ordinal columns to 0-indexed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 396,
   "id": "c33cac36",
   "metadata": {},
   "outputs": [],
   "source": [
    "def shift_to_zero_indexed(df, columns):\n",
    "    \"\"\"\n",
    "    Shift ordinal categorical columns that start at 1 to start at 0.\n",
    "    \n",
    "    Args:\n",
    "        df: DataFrame to modify\n",
    "        columns: List of columns to check and shift\n",
    "    \n",
    "    Returns:\n",
    "        df: Modified DataFrame\n",
    "        shifted_cols: List of columns that were shifted\n",
    "    \"\"\"\n",
    "    shifted_cols = []\n",
    "    \n",
    "    for col in columns:\n",
    "        if col not in df.columns:\n",
    "            continue\n",
    "            \n",
    "        # Find minimum value, ignoring NaNs\n",
    "        min_val = df[col].dropna().min()\n",
    "        \n",
    "        # If minimum value is 1 (or higher), shift down\n",
    "        if min_val >= 1:\n",
    "            df[col] = df[col] - min_val\n",
    "            shifted_cols.append(col)\n",
    "    \n",
    "    print(f\"Shifted {len(shifted_cols)} columns to 0-indexed\")\n",
    "    return df, shifted_cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 397,
   "id": "69456491",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shifted 13 columns to 0-indexed\n"
     ]
    }
   ],
   "source": [
    "df_depression_data, shifted_cols = shift_to_zero_indexed(df_depression_data, ordinal_cols)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b817139e",
   "metadata": {},
   "source": [
    "### Preprocessing pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 398,
   "id": "161dc639",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import RobustScaler, OneHotEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 399,
   "id": "4fccc5aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessor = ColumnTransformer([\n",
    "    ('num', Pipeline([\n",
    "        ('imputer', SimpleImputer(strategy='median')),\n",
    "        ('scaler', RobustScaler())\n",
    "    ]), numerical_cols),\n",
    "    \n",
    "    ('ord', Pipeline([\n",
    "        ('imputer', SimpleImputer(strategy='most_frequent')),\n",
    "    ]), ordinal_cols),\n",
    "    \n",
    "    ('nom', Pipeline([\n",
    "        ('imputer', SimpleImputer(strategy='most_frequent')),\n",
    "        ('onehot', OneHotEncoder(drop='first', handle_unknown='ignore'))\n",
    "    ]), nominal_cols),\n",
    "    \n",
    "    ('bin', Pipeline([\n",
    "        ('imputer', SimpleImputer(strategy='most_frequent')),\n",
    "    ]), binary_cols),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 400,
   "id": "1f74a883",
   "metadata": {},
   "outputs": [],
   "source": [
    "target_cols = ['DPQ010', 'DPQ020', 'DPQ030', 'DPQ040', \n",
    "               'DPQ050', 'DPQ060', 'DPQ070', 'DPQ080', \n",
    "               'DPQ090', 'DPQ100']\n",
    "y = df_depression_data[target_cols].copy()\n",
    "X = df_depression_data.drop(columns=target_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 401,
   "id": "102d5bd5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\irene\\anaconda3\\envs\\eslEnv\\lib\\site-packages\\sklearn\\impute\\_base.py:653: UserWarning: Skipping features without any observed values: ['SLQ300' 'SLQ310' 'SLQ320' 'SLQ330']. At least one non-missing value is needed for imputation with strategy='median'.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[ 1. ,  0. ,  1. , ...,  0. ,  1. ,  0. ],\n",
       "       [-1. ,  0.5, -1. , ...,  0. ,  0. ,  1. ],\n",
       "       [ 0. , -1. , -1. , ...,  0. ,  1. ,  0. ],\n",
       "       ...,\n",
       "       [ 4. ,  0. , -0.5, ...,  0. ,  1. ,  1. ],\n",
       "       [ 0. ,  0. , -1.5, ...,  0. ,  1. ,  1. ],\n",
       "       [ 1. ,  0. ,  1. , ...,  0. ,  1. ,  1. ]], shape=(4167, 106))"
      ]
     },
     "execution_count": 401,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preprocessor.fit_transform(X)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "eslEnv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
