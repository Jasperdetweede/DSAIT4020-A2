{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "50ec7ef5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import sklearn\n",
    "\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6cca7c4",
   "metadata": {},
   "source": [
    "### Data Loading & Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19a66f62-a8c5-4e74-a621-ac11f181e31d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['ACD010A', 'ACD010B', 'ACD010C', 'ACD040', 'ALQ111', 'ALQ121', 'ALQ130', 'ALQ142', 'ALQ270', 'ALQ280', 'ALQ151', 'ALQ170', 'BPQ020', 'BPQ030', 'BPQ150', 'BPQ080', 'BPQ101D', 'DBQ010', 'DBD030', 'DBD041', 'DBD050', 'DBD055', 'DBD061', 'DBQ073A', 'DBQ073B', 'DBQ073C', 'DBQ073D', 'DBQ073E', 'DBQ073U', 'DBQ301', 'DBQ330', 'DBQ360', 'DBQ370', 'DBD381', 'DBQ390', 'DBQ400', 'DBD411', 'DBQ421', 'DBQ424', 'DBQ930', 'DBQ935', 'DBQ940', 'DBQ945', 'DIQ010', 'DID040', 'DIQ160', 'DIQ180', 'DIQ050', 'DID060', 'DIQ060U', 'DIQ070', 'FNQ021', 'FNQ041', 'FNQ050', 'FNQ060', 'FNQ080', 'FNQ160', 'FNQ100', 'FNQ110', 'FNQ120', 'FNQ170', 'FNQ180', 'FNQ190', 'FNQ130', 'FNQ200', 'FNQ140', 'FNQ150', 'FNDCDI', 'FNQ410', 'FNQ430', 'FNQ440', 'FNQ450', 'FNQ460', 'FNQ470', 'FNQ480', 'FNQ490', 'FNQ510', 'FNQ520', 'FNQ530', 'FNQ540', 'FNDADI', 'FNDAEDI', 'FSD032A', 'FSD032B', 'FSD032C', 'FSD041', 'FSD052', 'FSD061', 'FSD071', 'FSD081', 'FSD092', 'FSD102', 'FSDAD', 'FSD151', 'FSQ165', 'FSD165N', 'FSQ012', 'FSD012N', 'FSD230', 'FSD230N', 'FSD795', 'FSD225', 'FSD235', 'FSD162', 'FSQ760', 'FSD760N', 'FSQ653', 'FSD660ZC', 'FSD675', 'FSD680', 'FSD670ZC', 'FSQ690', 'FSQ695', 'HIQ011', 'HIQ032A', 'HIQ032B', 'HIQ032C', 'HIQ032D', 'HIQ032E', 'HIQ032F', 'HIQ032H', 'HIQ032I', 'HIQ210', 'HOD051', 'HSQ590', 'HUQ010', 'HUQ030', 'HUQ042', 'HUQ055', 'HUQ090', 'INDFMMPI', 'INDFMMPC', 'INQ300', 'IND310', 'KIQ022', 'KIQ025', 'KIQ005', 'KIQ010', 'KIQ042', 'KIQ044', 'KIQ052', 'KIQ481', 'MCQ010', 'MCQ035', 'MCQ040', 'MCQ050', 'AGQ030', 'MCQ053', 'MCQ149', 'MCQ160A', 'MCQ195', 'MCQ160B', 'MCQ160C', 'MCQ160D', 'MCQ160E', 'MCQ160F', 'MCQ160M', 'MCQ170M', 'MCQ160P', 'MCQ160L', 'MCQ170L', 'MCQ500', 'MCQ510A', 'MCQ510B', 'MCQ510C', 'MCQ510D', 'MCQ510E', 'MCQ510F', 'MCQ550', 'MCQ560', 'MCQ220', 'MCQ230A', 'MCQ230B', 'MCQ230C', 'MCQ230D', 'OSQ230', 'OCD150', 'OCQ180', 'OCQ210', 'OCQ215', 'OCQ383', 'OHQ845', 'OHQ620', 'OHQ630', 'OHQ640', 'OHQ660', 'OHQ670', 'OHQ680', 'PAQ706', 'PAQ711', 'PAD790Q', 'PAD790U', 'PAD800', 'PAD810Q', 'PAD810U', 'PAD820', 'PAD680', 'RHQ010', 'RHQ031', 'RHD043', 'RHQ060', 'RHQ078', 'RHQ131', 'RHD143', 'RHD167', 'RHQ200', 'RHD280', 'RHQ305', 'RHQ332', 'RXQ510', 'RXQ515', 'RXQ520', 'RXQ033', 'RXQ050', 'SLQ300', 'SLQ310', 'SLD012', 'SLQ320', 'SLQ330', 'SLD013', 'SMD460', 'SMD470', 'SMQ681', 'SMQ690A', 'SMQ710', 'SMQ720', 'SMQ725', 'SMQ690B', 'SMQ740', 'SMQ690C', 'SMQ770', 'SMQ690G', 'SMQ845', 'SMQ846', 'SMQ849', 'SMQ851', 'SMQ690D', 'SMQ690E', 'SMQ690K', 'SMQ863', 'SMQ690F', 'SMDANY', 'SMQ020', 'SMQ040', 'SMD641', 'SMD650', 'SMD100MN', 'SMQ621', 'SMD630', 'SMAQUEX2', 'WHD010', 'WHD020', 'WHD050', 'WHQ070']\n"
     ]
    }
   ],
   "source": [
    "# Read dataset from csv\n",
    "dataset = pd.read_csv('processed_data/depression_data.csv')\n",
    "target_embed_cols = ['DPQ010','DPQ020','DPQ030','DPQ040','DPQ050','DPQ060','DPQ070','DPQ080','DPQ090']\n",
    "\n",
    "# Add binary target column based on DSM-V criteria\n",
    "depression_criteria = (\n",
    "    (dataset['DPQ010'].isin([2, 3]) | dataset['DPQ020'].isin([2, 3])) &     # Little interest in doing things OR feeling down more than half the days\n",
    "    (dataset[target_embed_cols].isin([2, 3]).sum(axis=1) >= 5)              # At least 5 symptoms present more than half the days \n",
    ")\n",
    "\n",
    "dataset['depressed'] = (depression_criteria).astype(int)\n",
    "\n",
    "# Get features from the depression file, so we can drop them in X\n",
    "features_from_depression_file = list(pd.read_sas('raw_data/targets/DPQ_L_Target_Depression.xpt', format='xport').drop(columns='SEQN').columns)\n",
    "\n",
    "# Define sets\n",
    "X = dataset.drop(columns=features_from_depression_file).drop(columns=['SEQN', 'depressed'])\n",
    "y_embed = dataset[target_embed_cols]\n",
    "y_binary = dataset['depressed']\n",
    "\n",
    "# Global variables\n",
    "STATE = 42"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "ff900163",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assert correct shape and absence of SEQN column in features and targets\n",
    "assert X.shape[0] == y_embed.shape[0], \"Feature and target embedding row counts do not match\"\n",
    "assert X.shape[0] == y_binary.shape[0], \"Feature and target binary row counts do not match\"\n",
    "assert X.columns.__contains__(\"SEQN\") == False, \"Feature set should not contain SEQN column\"\n",
    "assert y_embed.columns.__contains__(\"SEQN\") == False, \"Target embedding set should not contain SEQN column\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "c6594972",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split into train and test sets \n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X,\n",
    "    y_binary,\n",
    "    test_size=0.2,        \n",
    "    random_state=STATE,   \n",
    "    stratify=y_binary       # preserve class balance\n",
    ")\n",
    "\n",
    "y_embed_train = y_embed.loc[X_train.index]\n",
    "y_embed_test  = y_embed.loc[X_test.index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1f2b1a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dropped 0 columns with >50% missing. Remaining: 113 columns (was 113)\n",
      "Dropped 0 rows with >50% missing. Remaining: 4167 rows (was 4167)\n",
      "Categorical features: ['PAD790U', 'PAD810U', 'SLQ300', 'SLQ310', 'SLQ320', 'SLQ330']\n",
      "Numeric features: ['ACD010A', 'ALQ111', 'ALQ121', 'ALQ130', 'ALQ142', 'ALQ151', 'BPQ020', 'BPQ080', 'BPQ101D', 'DBQ930', 'DBQ935', 'DBQ940', 'DBQ945', 'DIQ010', 'DIQ160', 'DIQ180', 'FNQ410', 'FNQ430', 'FNQ440', 'FNQ450', 'FNQ460', 'FNQ470', 'FNQ480', 'FNQ490', 'FNQ510', 'FNQ520', 'FNQ530', 'FNQ540', 'FNDADI', 'FNDAEDI', 'FSD032A', 'FSD032B', 'FSD032C', 'FSDAD', 'FSD151', 'FSQ165', 'FSD162', 'HIQ011', 'HIQ032A', 'HIQ210', 'HOD051', 'HSQ590', 'HUQ010', 'HUQ030', 'HUQ042', 'HUQ055', 'HUQ090', 'INDFMMPI', 'INDFMMPC', 'INQ300', 'KIQ022', 'KIQ005', 'KIQ042', 'KIQ044', 'KIQ052', 'KIQ481', 'MCQ010', 'AGQ030', 'MCQ053', 'MCQ160A', 'MCQ160B', 'MCQ160C', 'MCQ160D', 'MCQ160E', 'MCQ160F', 'MCQ160M', 'MCQ160P', 'MCQ160L', 'MCQ550', 'MCQ560', 'MCQ220', 'OSQ230', 'OCD150', 'OCQ180', 'OCQ215', 'OHQ845', 'OHQ620', 'OHQ630', 'OHQ640', 'OHQ660', 'OHQ670', 'OHQ680', 'PAD790Q', 'PAD800', 'PAD810Q', 'PAD680', 'RHQ010', 'RHQ031', 'RHD280', 'RHQ305', 'RXQ510', 'RXQ033', 'RXQ050', 'SLD012', 'SLD013', 'SMD460', 'SMQ681', 'SMQ846', 'SMQ851', 'SMQ863', 'SMDANY', 'SMQ020', 'SMAQUEX2', 'WHD010', 'WHD020', 'WHD050', 'WHQ070']\n",
      "Preprocessed feature matrix shape: (3333, 365)\n",
      "Target columns in X: set()\n"
     ]
    }
   ],
   "source": [
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "# Columns with >50% missing\n",
    "cols_before = X.shape[1]\n",
    "cols_to_drop = X.columns[X.isnull().mean() > 0.5]\n",
    "X = X.drop(columns=cols_to_drop)\n",
    "print(f\"Dropped {len(cols_to_drop)} columns with >50% missing. Remaining: {X.shape[1]} columns (was {cols_before})\")\n",
    "\n",
    "# Rows with >50% missing\n",
    "rows_before = X.shape[0]\n",
    "rows_to_drop = X.index[X.isnull().mean(axis=1) > 0.5]\n",
    "X = X.drop(index=rows_to_drop)\n",
    "y_binary = y_binary.loc[X.index]  # Align target\n",
    "y_embed = y_embed.loc[X.index]\n",
    "print(f\"Dropped {len(rows_to_drop)} rows with >50% missing. Remaining: {X.shape[0]} rows (was {rows_before})\")\n",
    "\n",
    "# Identify categorical vs numeric\n",
    "categorical_cols = X.select_dtypes(include=['object', 'category']).columns.tolist()\n",
    "numeric_cols = X.select_dtypes(include=['number']).columns.tolist()\n",
    "print(f\"Categorical features: {categorical_cols}\")\n",
    "print(f\"Numeric features: {numeric_cols}\")\n",
    "\n",
    "# Column transformer with imputation\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        # Numeric pipeline: median imputation → scaling\n",
    "        ('num', Pipeline([\n",
    "            ('imputer', SimpleImputer(strategy='median')),\n",
    "            ('scaler', StandardScaler())\n",
    "        ]), numeric_cols),\n",
    "        \n",
    "        # Categorical pipeline: most frequent imputation → one-hot encoding\n",
    "        ('cat', Pipeline([\n",
    "            ('imputer', SimpleImputer(strategy='most_frequent')),\n",
    "            ('onehot', OneHotEncoder(handle_unknown='ignore'))\n",
    "        ]), categorical_cols)\n",
    "    ]\n",
    ")\n",
    "\n",
    "preprocessor.fit(X_train)\n",
    "\n",
    "X_train_preprocessed = preprocessor.transform(X_train)\n",
    "X_test_preprocessed  = preprocessor.transform(X_test)\n",
    "print(f\"Preprocessed feature matrix shape: {X_train_preprocessed.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "756a554e",
   "metadata": {},
   "source": [
    "### Random forests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b249968e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# All imports only used in this subsection\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import f1_score, roc_auc_score, classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77f591c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 score: 0.25\n",
      "ROC-AUC: 0.6013996719518863\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.94      0.93      0.94       775\n",
      "           1       0.23      0.27      0.25        59\n",
      "\n",
      "    accuracy                           0.88       834\n",
      "   macro avg       0.59      0.60      0.59       834\n",
      "weighted avg       0.89      0.88      0.89       834\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Baseline\n",
    "rfc_baseline_model = RandomForestClassifier(n_estimators=1, random_state=STATE)\n",
    "rfc_baseline_model.fit(X_train_preprocessed, y_train)\n",
    "\n",
    "y_pred = rfc_baseline_model.predict(X_test_preprocessed)\n",
    "y_proba = rfc_baseline_model.predict_proba(X_test_preprocessed)[:, 1]\n",
    "\n",
    "print(\"F1 score:\", f1_score(y_test, y_pred))\n",
    "print(\"ROC-AUC:\", roc_auc_score(y_test, y_proba))\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff087dc0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE per DPQ item: [0.58665767 0.49892269 0.83707749 0.67195899 0.68924811 0.59399463\n",
      " 0.65525156 0.45110989 0.18155387]\n",
      "Average MSE: 0.5739749900079936\n"
     ]
    }
   ],
   "source": [
    "# Ours\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "\n",
    "# # Multi-output Random Forest\n",
    "# rfr = RandomForestRegressor(\n",
    "#     n_estimators=200,\n",
    "#     random_state=STATE,\n",
    "#     n_jobs=-1\n",
    "# )\n",
    "\n",
    "# rfr.fit(X_train_preprocessed, y_embed_train)\n",
    "\n",
    "# # Predict\n",
    "# y_pred_embed = rfr.predict(X_test_preprocessed)\n",
    "\n",
    "# # Evaluate\n",
    "# from sklearn.metrics import mean_squared_error\n",
    "\n",
    "# mse_per_item = mean_squared_error(y_embed_test, y_pred_embed, multioutput='raw_values')\n",
    "# print(\"MSE per DPQ item:\", mse_per_item)\n",
    "# print(\"Average MSE:\", mse_per_item.mean())\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4966ccdd",
   "metadata": {},
   "source": [
    "### Logistic Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62e65164",
   "metadata": {},
   "source": [
    "### Bayesian Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4607f69",
   "metadata": {},
   "source": [
    "### MLP (with torch)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv (3.12.10)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
