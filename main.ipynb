{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cae6b255",
   "metadata": {},
   "source": [
    "# **Initialization** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "250f29be",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "656d0b61",
   "metadata": {},
   "source": [
    "### Control"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cce58cb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Paths\n",
    "RAW_DATA_FOLDER = 'raw_data'\n",
    "TARGET_FILE_PATH = 'unprocessed_data'\n",
    "\n",
    "# Flow Controls\n",
    "RELOAD_RAW_DATA = False\n",
    "DO_SMOTE = True\n",
    "DATA = 'depression'  # Options: 'depression', 'insomnia', 'electrical_circuit'\n",
    "\n",
    "# System variables\n",
    "STATE = 42\n",
    "TEST_SET_FRACTION = 0.20\n",
    "MISSING_VALUES_THRESHOLD = 0.50\n",
    "SAMPLES_ELECTRICAL_CIRCUIT = 5000\n",
    "VERBOSE = True\n",
    "FLIP_LABEL_FRACTION = 0.03\n",
    "\n",
    "np.random.seed(STATE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aeeb00e7",
   "metadata": {},
   "source": [
    "# **Data Preparation**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3601f28b",
   "metadata": {},
   "source": [
    "### Merge raw data files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "be2998a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from raw_data_loader import load_raw_data\n",
    "\n",
    "if (RELOAD_RAW_DATA):\n",
    "    load_raw_data(RAW_DATA_FOLDER, TARGET_FILE_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8460a91b",
   "metadata": {},
   "source": [
    "### Preprocessing and Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a9f59207",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dropped 142 columns with >50.0% missing values\n",
      "Shape after dropping high-missing columns: (3333, 109)\n",
      "Replaced 708 special code values with NaN\n",
      "Replaced 186 special code values with NaN\n",
      "Ordinal columns: 27\n",
      "Nominal columns: 3\n",
      "Binary columns: 2\n",
      "Numerical columns: 74\n",
      "Object columns (excluded): 3\n",
      "Total columns identified: 109\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib/python3.14/site-packages/sklearn/impute/_base.py:641: UserWarning: Skipping features without any observed values: ['SLQ300' 'SLQ310' 'SLQ320' 'SLQ330']. At least one non-missing value is needed for imputation with strategy='median'.\n",
      "  warnings.warn(\n",
      "/usr/lib/python3.14/site-packages/sklearn/impute/_base.py:641: UserWarning: Skipping features without any observed values: ['SLQ300' 'SLQ310' 'SLQ320' 'SLQ330']. At least one non-missing value is needed for imputation with strategy='median'.\n",
      "  warnings.warn(\n",
      "/usr/lib/python3.14/site-packages/sklearn/impute/_base.py:641: UserWarning: Skipping features without any observed values: ['SLQ300' 'SLQ310' 'SLQ320' 'SLQ330']. At least one non-missing value is needed for imputation with strategy='median'.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from preprocessing_depression import clean_and_preprocess_depression_data\n",
    "from preprocessing_insomnia import clean_and_preprocess_insomnia_data\n",
    "from preprocessing_electrical_circuit import gen_and_preprocess_ec_data\n",
    "\n",
    "dataset = pd.read_csv(TARGET_FILE_PATH + '/depression_data.csv')\n",
    "\n",
    "if DATA == 'depression':\n",
    "    X_train, X_test, y_train, y_test, y_embed_train, y_embed_test = clean_and_preprocess_depression_data(dataset, RAW_DATA_FOLDER, TEST_SET_FRACTION, STATE, MISSING_VALUES_THRESHOLD)\n",
    "elif DATA == 'insomnia':\n",
    "    X_train, X_test, y_train, y_test, y_embed_train, y_embed_test = clean_and_preprocess_insomnia_data(dataset, RAW_DATA_FOLDER, TEST_SET_FRACTION, STATE, MISSING_VALUES_THRESHOLD)\n",
    "elif DATA == 'electrical_circuit':\n",
    "    X_train, X_test, y_train, y_test, y_embed_train, y_embed_test = gen_and_preprocess_ec_data(SAMPLES_ELECTRICAL_CIRCUIT, TEST_SET_FRACTION, STATE)\n",
    "    DO_SMOTE = False\n",
    "else:\n",
    "    raise ValueError(\"Invalid dataset selected\")\n",
    "\n",
    "#TODO Fix issue with the time columns SLQ300/310/320/330 in depression and processing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43f43add",
   "metadata": {},
   "source": [
    "### Data Balancing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5bad909f",
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name '_is_pandas_df' from 'sklearn.utils.validation' (/usr/lib/python3.14/site-packages/sklearn/utils/validation.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mImportError\u001b[39m                               Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[5]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mdata_balancing\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m resample_training_data\n\u001b[32m      3\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m DO_SMOTE:\n\u001b[32m      4\u001b[39m     X_train, y_train, y_embed_train = resample_training_data(X_train, y_train, y_embed_train, random_state=STATE)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Education/Formal/TuDelft/DSAIT 4020 - Elements of Statistical Learning/Assignment-2/data_balancing.py:3\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnumpy\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnp\u001b[39;00m\n\u001b[32m      2\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpandas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpd\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m3\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mimblearn\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mover_sampling\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m SMOTE, ADASYN\n\u001b[32m      4\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mimblearn\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01munder_sampling\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m RandomUnderSampler\n\u001b[32m      5\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mimblearn\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mcombine\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m SMOTETomek\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/lib/python3.14/site-packages/imblearn/__init__.py:54\u001b[39m\n\u001b[32m     50\u001b[39m     sys.stderr.write(\u001b[33m\"\u001b[39m\u001b[33mPartial import of imblearn during the build process.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m     51\u001b[39m     \u001b[38;5;66;03m# We are not importing the rest of scikit-learn during the build\u001b[39;00m\n\u001b[32m     52\u001b[39m     \u001b[38;5;66;03m# process, as it may not be compiled yet\u001b[39;00m\n\u001b[32m     53\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m54\u001b[39m     \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[32m     55\u001b[39m         combine,\n\u001b[32m     56\u001b[39m         ensemble,\n\u001b[32m     57\u001b[39m         exceptions,\n\u001b[32m     58\u001b[39m         metrics,\n\u001b[32m     59\u001b[39m         model_selection,\n\u001b[32m     60\u001b[39m         over_sampling,\n\u001b[32m     61\u001b[39m         pipeline,\n\u001b[32m     62\u001b[39m         tensorflow,\n\u001b[32m     63\u001b[39m         under_sampling,\n\u001b[32m     64\u001b[39m         utils,\n\u001b[32m     65\u001b[39m     )\n\u001b[32m     66\u001b[39m     \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01m_version\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m __version__\n\u001b[32m     67\u001b[39m     \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mbase\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m FunctionSampler\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/lib/python3.14/site-packages/imblearn/combine/__init__.py:5\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[33;03m\"\"\"The :mod:`imblearn.combine` provides methods which combine\u001b[39;00m\n\u001b[32m      2\u001b[39m \u001b[33;03mover-sampling and under-sampling.\u001b[39;00m\n\u001b[32m      3\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m5\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mimblearn\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mcombine\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01m_smote_enn\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m SMOTEENN\n\u001b[32m      6\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mimblearn\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mcombine\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01m_smote_tomek\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m SMOTETomek\n\u001b[32m      8\u001b[39m __all__ = [\u001b[33m\"\u001b[39m\u001b[33mSMOTEENN\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mSMOTETomek\u001b[39m\u001b[33m\"\u001b[39m]\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/lib/python3.14/site-packages/imblearn/combine/_smote_enn.py:12\u001b[39m\n\u001b[32m      9\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01msklearn\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mbase\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m clone\n\u001b[32m     10\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01msklearn\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mutils\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m check_X_y\n\u001b[32m---> \u001b[39m\u001b[32m12\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mimblearn\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mbase\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m BaseSampler\n\u001b[32m     13\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mimblearn\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mover_sampling\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m SMOTE\n\u001b[32m     14\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mimblearn\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mover_sampling\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mbase\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m BaseOverSampler\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/lib/python3.14/site-packages/imblearn/base.py:14\u001b[39m\n\u001b[32m     12\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01msklearn\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mutils\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01m_metadata_requests\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m METHODS\n\u001b[32m     13\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01msklearn\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mutils\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mmulticlass\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m check_classification_targets\n\u001b[32m---> \u001b[39m\u001b[32m14\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01msklearn_compat\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mbase\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m _fit_context\n\u001b[32m     15\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01msklearn_compat\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mutils\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mvalidation\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m validate_data\n\u001b[32m     17\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mimblearn\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mutils\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m check_sampling_strategy, check_target_type\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/lib/python3.14/site-packages/sklearn_compat/base.py:1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01msklearn_compat\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01m_sklearn_compat\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[32m      2\u001b[39m     _fit_context,  \u001b[38;5;66;03m# noqa: F401\u001b[39;00m\n\u001b[32m      3\u001b[39m     is_clusterer,  \u001b[38;5;66;03m# noqa: F401\u001b[39;00m\n\u001b[32m      4\u001b[39m )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/lib/python3.14/site-packages/sklearn_compat/_sklearn_compat.py:258\u001b[39m\n\u001b[32m    253\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    254\u001b[39m     \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01msklearn\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mutils\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mmetadata_routing\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[32m    255\u001b[39m         _raise_for_params,  \u001b[38;5;66;03m# noqa: F401\u001b[39;00m\n\u001b[32m    256\u001b[39m         process_routing,  \u001b[38;5;66;03m# noqa: F401\u001b[39;00m\n\u001b[32m    257\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m258\u001b[39m     \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01msklearn\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mutils\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mvalidation\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[32m    259\u001b[39m         _is_fitted,  \u001b[38;5;66;03m# noqa: F401\u001b[39;00m\n\u001b[32m    260\u001b[39m         _is_pandas_df,  \u001b[38;5;66;03m# noqa: F401\u001b[39;00m\n\u001b[32m    261\u001b[39m     )\n\u001b[32m    264\u001b[39m \u001b[38;5;66;03m########################################################################################\u001b[39;00m\n\u001b[32m    265\u001b[39m \u001b[38;5;66;03m# Upgrading for scikit-learn 1.5\u001b[39;00m\n\u001b[32m    266\u001b[39m \u001b[38;5;66;03m########################################################################################\u001b[39;00m\n\u001b[32m    269\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m sklearn_version < parse_version(\u001b[33m\"\u001b[39m\u001b[33m1.5\u001b[39m\u001b[33m\"\u001b[39m):\n\u001b[32m    270\u001b[39m     \u001b[38;5;66;03m# chunking\u001b[39;00m\n\u001b[32m    271\u001b[39m     \u001b[38;5;66;03m# extmath\u001b[39;00m\n\u001b[32m    272\u001b[39m     \u001b[38;5;66;03m# fixes\u001b[39;00m\n",
      "\u001b[31mImportError\u001b[39m: cannot import name '_is_pandas_df' from 'sklearn.utils.validation' (/usr/lib/python3.14/site-packages/sklearn/utils/validation.py)"
     ]
    }
   ],
   "source": [
    "from data_balancing import resample_training_data\n",
    "\n",
    "if DO_SMOTE:\n",
    "    X_train, y_train, y_embed_train = resample_training_data(X_train, y_train, y_embed_train, random_state=STATE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "23d5ad2c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class Distribution:\n",
      " {np.int64(0): np.int64(3095), np.int64(1): np.int64(238)}\n",
      "\n",
      "Class ratio: 0.077\n"
     ]
    }
   ],
   "source": [
    "# Check class distribution\n",
    "classes, counts = np.unique(y_train, return_counts=True)\n",
    "print(\"Class Distribution:\\n\", dict(zip(classes, counts)))\n",
    "\n",
    "if len(classes) > 1:\n",
    "    class_ratio = counts[1] / counts[0]\n",
    "    print(f\"\\nClass ratio: {class_ratio:.3f}\")\n",
    "else:\n",
    "    print(\"\\nOnly one class present.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "faf1687a-66bf-4c13-ae9d-b9f729255aae",
   "metadata": {},
   "source": [
    "### Introduce Noise to label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8e605cc0-c7df-46fa-aafc-72d22d688dbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "assert FLIP_LABEL_FRACTION > 0.0 and FLIP_LABEL_FRACTION < 1.0, \"FLIP_LABEL_FRACTION should be beween 0.0 and 1.0\"\n",
    "\n",
    "# Randomly select indices to flip\n",
    "if FLIP_LABEL_FRACTION > 0.0:\n",
    "    num_to_flip = int(FLIP_LABEL_FRACTION * len(y_train))\n",
    "    flip_indices = np.random.choice(len(y_train), size=num_to_flip, replace=False)\n",
    "\n",
    "    # If y_train is a pandas Series, convert to int for safe arithmetic\n",
    "    if hasattr(y_train, 'iloc'):\n",
    "        y_train = y_train.astype(int)\n",
    "        y_train.iloc[flip_indices] = 1 - y_train.iloc[flip_indices]\n",
    "    else:  # numpy array\n",
    "        y_train[flip_indices] = 1 - y_train[flip_indices]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "649caa02-af7b-4ed1-8893-ed4192ddc29d",
   "metadata": {},
   "source": [
    "### Make everything a numpy array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "47910799-d5c6-4eb2-b8d2-fe5b597640b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = X_train.values if hasattr(X_train, \"values\") else np.array(X_train)\n",
    "X_test = X_test.values if hasattr(X_test, \"values\") else np.array(X_test)\n",
    "\n",
    "y_train = y_train.values.ravel() if hasattr(y_train, \"values\") else np.array(y_train).ravel()\n",
    "y_test = y_test.values.ravel() if hasattr(y_test, \"values\") else np.array(y_test).ravel()\n",
    "\n",
    "y_embed_train = y_embed_train.values if hasattr(y_embed_train, \"values\") else np.array(y_embed_train)\n",
    "y_embed_test = y_embed_test.values if hasattr(y_embed_test, \"values\") else np.array(y_embed_test)\n",
    "\n",
    "assert(isinstance(X_train, np.ndarray))\n",
    "assert(isinstance(X_test, np.ndarray))\n",
    "assert(isinstance(y_train, np.ndarray))\n",
    "assert(isinstance(y_test, np.ndarray))\n",
    "assert(isinstance(y_embed_train, np.ndarray))\n",
    "assert(isinstance(y_embed_test, np.ndarray))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "907be545",
   "metadata": {},
   "source": [
    "# **Models**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2b23470a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from baseline_models import train_multitarget_baseline\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import GaussianNB"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3383a4b1",
   "metadata": {},
   "source": [
    "### Training Bayesian Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "33a6ac8b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      " ######################################## GaussianNB Multitarget Regressor ########################################\n",
      "Train MSE per embedding: [ 52059.36025  41509.8905   54759.43225  63583.644    77929.185\n",
      "  62877.18275 107183.147    47843.55025]\n",
      "Test MSE per embedding: [ 56894.288  44889.096  63442.078  74432.063  90656.514  77874.897\n",
      " 119260.47   49646.971]\n",
      "Average train MSE: 63468.174\n",
      "Average test MSE: 72137.047125\n"
     ]
    }
   ],
   "source": [
    "nb_model = GaussianNB()\n",
    "y_pred_nb, acc_nb = train_multitarget_baseline(\n",
    "                            model=nb_model,\n",
    "                            is_classifier=False,\n",
    "                            X_train=X_train,\n",
    "                            X_test=X_test,\n",
    "                            y_embed_train=y_embed_train,\n",
    "                            y_embed_test=y_embed_test,\n",
    "                            verbose=VERBOSE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90ac3cc7",
   "metadata": {},
   "source": [
    "### Training Random Forests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "30e979e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameter\n",
    "N_ESTIMATORS = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "dbaeae85",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      " ######################################## RandomForestRegressor Multitarget Regressor ########################################\n",
      "Train MSE per embedding: [ 495.97067715  696.42313045  593.53877665  391.28257722  123.41573847\n",
      " 3346.21772207 5408.1000016   502.34868655]\n",
      "Test MSE per embedding: [ 2943.6461006  5329.0526277  4048.9118285  2540.1975794   734.7480725\n",
      " 24310.930406  40204.2232533  3802.708152 ]\n",
      "Average train MSE: 1444.6621637718752\n",
      "Average test MSE: 10489.302252499998\n"
     ]
    }
   ],
   "source": [
    "rf_model = RandomForestRegressor(n_estimators=N_ESTIMATORS, random_state=STATE, n_jobs=-1)\n",
    "y_pred_rf, mse_rf = train_multitarget_baseline(\n",
    "                                    model=rf_model, \n",
    "                                    is_classifier=False, \n",
    "                                    X_train=X_train, \n",
    "                                    X_test=X_test, \n",
    "                                    y_embed_train=y_embed_train, \n",
    "                                    y_embed_test=y_embed_test,\n",
    "                                    verbose=VERBOSE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3172fa83",
   "metadata": {},
   "source": [
    "### Training Logistic Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f84084e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters\n",
    "MAX_ITERATIONS = 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c3635769",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      " ######################################## LogisticRegression Multitarget Classifier ########################################\n",
      "Train accuracy per embedding: [0.06525 0.069   0.06175 0.0595  0.0595  0.056   0.0525  0.061  ]\n",
      "Test accuracy per embedding: [0.003 0.003 0.001 0.001 0.002 0.001 0.001 0.002]\n",
      "Average train accuracy: 0.0605625\n",
      "Average test accuracy: 0.00175\n"
     ]
    }
   ],
   "source": [
    "log_model = LogisticRegression(max_iter=MAX_ITERATIONS, class_weight='balanced', random_state=STATE)\n",
    "y_pred_log, acc_log = train_multitarget_baseline(\n",
    "                            model=log_model,\n",
    "                            is_classifier=True,\n",
    "                            X_train=X_train,\n",
    "                            X_test=X_test,\n",
    "                            y_embed_train=y_embed_train,\n",
    "                            y_embed_test=y_embed_test,\n",
    "                            verbose=VERBOSE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b7acb7b-dd16-4725-96fe-5ba08b7a4259",
   "metadata": {},
   "source": [
    "## Proposed MLPs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "06a4020a-3be1-4625-9323-5a674670d6d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from proposed_models import train_joint_model, train_split_model, train_deep_joint_model, train_deep_split_model\n",
    "\n",
    "DEVICE = \"cpu\"#torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "E_KEEP_RATE = 0.7\n",
    "l = 1\n",
    "if DATA == 'depression':\n",
    "    l = 1e-2\n",
    "elif DATA == 'insomnia':\n",
    "    l = 1e-2\n",
    "elif DATA == 'electrical_circuit':\n",
    "    l = 1\n",
    "\n",
    "EPOCHS = 100\n",
    "AUGMENT_EPOCHS = EPOCHS//2\n",
    "EARLY_STOP_EPOCHS = EPOCHS//5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a932baae-9847-409c-b512-8255f7be4e8b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using  cpu  for torch\n"
     ]
    }
   ],
   "source": [
    "# Sanity Checks\n",
    "print(\"Using \", DEVICE, \" for torch\")\n",
    "\n",
    "assert X_train.shape[0] >= 100 and y_train.shape[0] >= 100 and y_embed_train.shape[0] >= 100, \"Arrays must have at least 100 samples for the check.\"\n",
    "\n",
    "aligned = (len(X_train[:100]) == len(y_train[:100])) and (len(X_train[:100]) == len(y_embed_train[:100]))\n",
    "assert aligned, \"First 100 samples of X_train, y_train, and y_embed_train are not aligned.\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "777438b4",
   "metadata": {},
   "source": [
    "### Train Joint MLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "aebf6f8e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training:\t###############################################---\t[95.0% - DONE]\n",
      "\n",
      "================================================================================\n",
      "================================   Joint MLP   ================================\n",
      "================================================================================\n",
      "Regression Results:\n",
      "MSE:\t0.6838266849517822\n",
      "\n",
      "\n",
      "Classification Results:\n",
      "F1 score: 0.25\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.94      0.93      0.94       775\n",
      "           1       0.23      0.27      0.25        59\n",
      "\n",
      "    accuracy                           0.88       834\n",
      "   macro avg       0.59      0.60      0.59       834\n",
      "weighted avg       0.89      0.88      0.89       834\n",
      "\n",
      "Confusion matrix:\n",
      " [[722  53]\n",
      " [ 43  16]]\n",
      "Training:\t##################################################\t[100.0%]\n",
      "\n",
      "================================================================================\n",
      "==========================   Joint MLP (Augmented)   ==========================\n",
      "================================================================================\n",
      "Regression Results:\n",
      "MSE:\t0.8706539869308472\n",
      "\n",
      "\n",
      "Classification Results:\n",
      "F1 score: 0.1415929203539823\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.93      0.94      0.94       775\n",
      "           1       0.15      0.14      0.14        59\n",
      "\n",
      "    accuracy                           0.88       834\n",
      "   macro avg       0.54      0.54      0.54       834\n",
      "weighted avg       0.88      0.88      0.88       834\n",
      "\n",
      "Confusion matrix:\n",
      " [[729  46]\n",
      " [ 51   8]]\n"
     ]
    }
   ],
   "source": [
    "train_joint_model( X_train, X_test, y_train, y_test, y_embed_train, y_embed_test,\n",
    "                    e_kept_ratio=E_KEEP_RATE,\n",
    "                    l=l,\n",
    "                    epochs=EPOCHS,\n",
    "                    augment_epochs=AUGMENT_EPOCHS,\n",
    "                    early_stop_epochs=EARLY_STOP_EPOCHS,\n",
    "                    device=DEVICE\n",
    "                  )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e3fbefb",
   "metadata": {},
   "source": [
    "### Train Split MLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "da297fb4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training:\t##################################################\t[100.0%]\n",
      "\n",
      "================================================================================\n",
      "================================   Split MLP   ================================\n",
      "================================================================================\n",
      "Regression Results:\n",
      "MSE:\t1.0288583040237427\n",
      "\n",
      "\n",
      "Classification Results:\n",
      "F1 score: 0.19047619047619047\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.94      0.98      0.96       775\n",
      "           1       0.32      0.14      0.19        59\n",
      "\n",
      "    accuracy                           0.92       834\n",
      "   macro avg       0.63      0.56      0.57       834\n",
      "weighted avg       0.89      0.92      0.90       834\n",
      "\n",
      "Confusion matrix:\n",
      " [[758  17]\n",
      " [ 51   8]]\n",
      "Training:\t##################################################\t[100.0%]\n",
      "\n",
      "================================================================================\n",
      "==========================   Split MLP (Augmented)   ==========================\n",
      "================================================================================\n",
      "Regression Results:\n",
      "MSE:\t3.390148401260376\n",
      "\n",
      "\n",
      "Classification Results:\n",
      "F1 score: 0.1797752808988764\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.94      0.97      0.95       775\n",
      "           1       0.27      0.14      0.18        59\n",
      "\n",
      "    accuracy                           0.91       834\n",
      "   macro avg       0.60      0.55      0.57       834\n",
      "weighted avg       0.89      0.91      0.90       834\n",
      "\n",
      "Confusion matrix:\n",
      " [[753  22]\n",
      " [ 51   8]]\n"
     ]
    }
   ],
   "source": [
    "train_split_model( X_train, X_test, y_train, y_test, y_embed_train, y_embed_test,\n",
    "                    e_kept_ratio=E_KEEP_RATE,\n",
    "                    epochs=EPOCHS,\n",
    "                    augment_epochs=AUGMENT_EPOCHS,\n",
    "                    early_stop_epochs=EARLY_STOP_EPOCHS,\n",
    "                    device=DEVICE\n",
    "                  )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d31cd9f3-7021-4891-8cad-f2e4a9385bc1",
   "metadata": {},
   "source": [
    "### Train Deep Joint Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "92d3f100-ec35-4495-9dbf-a8fa10722067",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training:\t######################################------------\t[77.0% - DONE]\n",
      "\n",
      "================================================================================\n",
      "==============================   Deep Joint MLP   ==============================\n",
      "================================================================================\n",
      "Regression Results:\n",
      "MSE:\t1.2267913818359375\n",
      "\n",
      "\n",
      "Classification Results:\n",
      "F1 score: 0.0\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.93      1.00      0.96       775\n",
      "           1       0.00      0.00      0.00        59\n",
      "\n",
      "    accuracy                           0.93       834\n",
      "   macro avg       0.46      0.50      0.48       834\n",
      "weighted avg       0.86      0.93      0.90       834\n",
      "\n",
      "Confusion matrix:\n",
      " [[775   0]\n",
      " [ 59   0]]\n",
      "Training:\t####----------------------------------------------\t[8.0%]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib/python3.14/site-packages/sklearn/metrics/_classification.py:1833: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", result.shape[0])\n",
      "/usr/lib/python3.14/site-packages/sklearn/metrics/_classification.py:1833: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", result.shape[0])\n",
      "/usr/lib/python3.14/site-packages/sklearn/metrics/_classification.py:1833: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", result.shape[0])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training:\t##################################################\t[100.0%]\n",
      "\n",
      "================================================================================\n",
      "========================   Deep Joint MLP (Augmented)   ========================\n",
      "================================================================================\n",
      "Regression Results:\n",
      "MSE:\t1.2267913818359375\n",
      "\n",
      "\n",
      "Classification Results:\n",
      "F1 score: 0.0\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.93      1.00      0.96       775\n",
      "           1       0.00      0.00      0.00        59\n",
      "\n",
      "    accuracy                           0.93       834\n",
      "   macro avg       0.46      0.50      0.48       834\n",
      "weighted avg       0.86      0.93      0.90       834\n",
      "\n",
      "Confusion matrix:\n",
      " [[775   0]\n",
      " [ 59   0]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib/python3.14/site-packages/sklearn/metrics/_classification.py:1833: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", result.shape[0])\n",
      "/usr/lib/python3.14/site-packages/sklearn/metrics/_classification.py:1833: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", result.shape[0])\n",
      "/usr/lib/python3.14/site-packages/sklearn/metrics/_classification.py:1833: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", result.shape[0])\n"
     ]
    }
   ],
   "source": [
    "train_deep_joint_model( X_train, X_test, y_train, y_test, y_embed_train, y_embed_test,\n",
    "                        e_kept_ratio=E_KEEP_RATE,\n",
    "                        l=l,\n",
    "                        epochs=EPOCHS,\n",
    "                        augment_epochs=AUGMENT_EPOCHS,\n",
    "                        early_stop_epochs=EARLY_STOP_EPOCHS,\n",
    "                        device=DEVICE\n",
    "                      )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7eae81c0-f941-48f9-9ee6-ab0b9837fdb8",
   "metadata": {},
   "source": [
    "### Train Deep Split Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9afe671-bc85-474e-a353-870b65329c0a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training:\t################----------------------------------\t[32.0%]"
     ]
    }
   ],
   "source": [
    "train_deep_split_model( X_train, X_test, y_train, y_test, y_embed_train, y_embed_test,\n",
    "                        e_kept_ratio=E_KEEP_RATE,\n",
    "                        epochs=EPOCHS,\n",
    "                        augment_epochs=AUGMENT_EPOCHS,\n",
    "                        early_stop_epochs=EARLY_STOP_EPOCHS,\n",
    "                        device=DEVICE\n",
    "                      )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
