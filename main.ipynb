{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cae6b255",
   "metadata": {},
   "source": [
    "# **Initialization** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "250f29be",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "656d0b61",
   "metadata": {},
   "source": [
    "### Control"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cce58cb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Paths\n",
    "RAW_DATA_FOLDER = 'raw_data'\n",
    "TARGET_FILE_PATH = 'unprocessed_data'\n",
    "\n",
    "# Flow Controls\n",
    "RELOAD_RAW_DATA = False\n",
    "\n",
    "# System variables\n",
    "STATE = 42\n",
    "TEST_SET_FRACTION = 0.20\n",
    "MISSING_VALUES_THRESHOLD = 0.50\n",
    "SAMPLES_ELECTRICAL_CIRCUIT = 5000"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aeeb00e7",
   "metadata": {},
   "source": [
    "# **Data Preparation**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3601f28b",
   "metadata": {},
   "source": [
    "### Merge raw data files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "be2998a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from raw_data_loader import load_raw_data\n",
    "\n",
    "if (RELOAD_RAW_DATA):\n",
    "    load_raw_data(RAW_DATA_FOLDER, TARGET_FILE_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8460a91b",
   "metadata": {},
   "source": [
    "### Preprocessing and Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9f59207",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dropped 142 columns with >50.0% missing values\n",
      "Shape after dropping high-missing columns: (3333, 109)\n",
      "Replaced 708 special code values with NaN\n",
      "Replaced 186 special code values with NaN\n",
      "Ordinal columns: 27\n",
      "Nominal columns: 3\n",
      "Binary columns: 2\n",
      "Numerical columns: 74\n",
      "Object columns (excluded): 3\n",
      "Total columns identified: 109\n",
      "Basic Electrical Circuit Simulator/Generator\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\jaspe\\Documents\\Github_projects\\DSAIT4020-A2\\venv\\Lib\\site-packages\\sklearn\\impute\\_base.py:641: UserWarning: Skipping features without any observed values: ['SLQ300' 'SLQ310' 'SLQ320' 'SLQ330']. At least one non-missing value is needed for imputation with strategy='median'.\n",
      "  warnings.warn(\n",
      "c:\\Users\\jaspe\\Documents\\Github_projects\\DSAIT4020-A2\\venv\\Lib\\site-packages\\sklearn\\impute\\_base.py:641: UserWarning: Skipping features without any observed values: ['SLQ300' 'SLQ310' 'SLQ320' 'SLQ330']. At least one non-missing value is needed for imputation with strategy='median'.\n",
      "  warnings.warn(\n",
      "c:\\Users\\jaspe\\Documents\\Github_projects\\DSAIT4020-A2\\venv\\Lib\\site-packages\\sklearn\\impute\\_base.py:641: UserWarning: Skipping features without any observed values: ['SLQ300' 'SLQ310' 'SLQ320' 'SLQ330']. At least one non-missing value is needed for imputation with strategy='median'.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Found input variables with inconsistent numbers of samples: [10, 5000]",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[13]\u001b[39m\u001b[32m, line 6\u001b[39m\n\u001b[32m      4\u001b[39m dataset = pd.read_csv(TARGET_FILE_PATH + \u001b[33m'\u001b[39m\u001b[33m/depression_data.csv\u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m      5\u001b[39m dep_X_train, dep_X_test, dep_y_train, dep_y_test, dep_y_embed_train, dep_y_embed_test = clean_and_preprocess_depression_data(dataset, RAW_DATA_FOLDER, TEST_SET_FRACTION, STATE, MISSING_VALUES_THRESHOLD)\n\u001b[32m----> \u001b[39m\u001b[32m6\u001b[39m ec_X_train, ec_X_test, ec_y_train, ec_y_test, ec_y_embed_train, ec_y_embed_test = \u001b[43mgen_and_preprocess_ec_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43mSAMPLES_ELECTRICAL_CIRCUIT\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mTEST_SET_FRACTION\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mSTATE\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     10\u001b[39m \u001b[38;5;66;03m#TODO Fix issue with the time columns SLQ300/310/320/330\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\jaspe\\Documents\\Github_projects\\DSAIT4020-A2\\preprocessing_electrical_circuit.py:27\u001b[39m, in \u001b[36mgen_and_preprocess_ec_data\u001b[39m\u001b[34m(sample_count, test_split, random_state)\u001b[39m\n\u001b[32m     16\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m'''\u001b[39;00m\n\u001b[32m     17\u001b[39m \u001b[33;03mTakes a dataframe and splits it into the training set, the corresponding target embeddings and calculates a binary label.\u001b[39;00m\n\u001b[32m     18\u001b[39m \u001b[33;03mImportantly there is no need for cleaning as the data is fully generated\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m     23\u001b[39m \u001b[33;03m:return: for train and test sets: X, y, e\u001b[39;00m\n\u001b[32m     24\u001b[39m \u001b[33;03m'''\u001b[39;00m\n\u001b[32m     26\u001b[39m \u001b[38;5;66;03m# Load train/test split\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m27\u001b[39m X_train, X_test, y_train, y_test, e_train, e_test = \u001b[43mgen_and_split_ec_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m \u001b[49m\u001b[43msample_count\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest_split\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrandom_state\u001b[49m\u001b[43m \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     29\u001b[39m \u001b[38;5;66;03m# Preprocess the features\u001b[39;00m\n\u001b[32m     30\u001b[39m X_train_preprocessed, X_test_preprocessed = preprocess_ec_data(X_train, X_test)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\jaspe\\Documents\\Github_projects\\DSAIT4020-A2\\preprocessing_electrical_circuit.py:53\u001b[39m, in \u001b[36mgen_and_split_ec_data\u001b[39m\u001b[34m(sample_count, test_split, random_state)\u001b[39m\n\u001b[32m     50\u001b[39m X, y = ec.gen_random_samples( sample_count = sample_count, random_state=random_state )\n\u001b[32m     51\u001b[39m e = ec.get_embedding()\n\u001b[32m---> \u001b[39m\u001b[32m53\u001b[39m X_train, X_test, y_train, y_test = \u001b[43mtrain_test_split\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     54\u001b[39m \u001b[43m\t\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     55\u001b[39m \u001b[43m\t\u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     56\u001b[39m \u001b[43m\t\u001b[49m\u001b[43mtest_size\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtest_split\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     57\u001b[39m \u001b[43m\t\u001b[49m\u001b[43mrandom_state\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrandom_state\u001b[49m\n\u001b[32m     58\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     60\u001b[39m e_train = e.loc[X_train.index]\n\u001b[32m     61\u001b[39m e_test  = e.loc[X_test.index]\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\jaspe\\Documents\\Github_projects\\DSAIT4020-A2\\venv\\Lib\\site-packages\\sklearn\\utils\\_param_validation.py:218\u001b[39m, in \u001b[36mvalidate_params.<locals>.decorator.<locals>.wrapper\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    212\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    213\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[32m    214\u001b[39m         skip_parameter_validation=(\n\u001b[32m    215\u001b[39m             prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[32m    216\u001b[39m         )\n\u001b[32m    217\u001b[39m     ):\n\u001b[32m--> \u001b[39m\u001b[32m218\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    219\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m InvalidParameterError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    220\u001b[39m     \u001b[38;5;66;03m# When the function is just a wrapper around an estimator, we allow\u001b[39;00m\n\u001b[32m    221\u001b[39m     \u001b[38;5;66;03m# the function to delegate validation to the estimator, but we replace\u001b[39;00m\n\u001b[32m    222\u001b[39m     \u001b[38;5;66;03m# the name of the estimator by the name of the function in the error\u001b[39;00m\n\u001b[32m    223\u001b[39m     \u001b[38;5;66;03m# message to avoid confusion.\u001b[39;00m\n\u001b[32m    224\u001b[39m     msg = re.sub(\n\u001b[32m    225\u001b[39m         \u001b[33mr\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mparameter of \u001b[39m\u001b[33m\\\u001b[39m\u001b[33mw+ must be\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m    226\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mparameter of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc.\u001b[34m__qualname__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m must be\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m    227\u001b[39m         \u001b[38;5;28mstr\u001b[39m(e),\n\u001b[32m    228\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\jaspe\\Documents\\Github_projects\\DSAIT4020-A2\\venv\\Lib\\site-packages\\sklearn\\model_selection\\_split.py:2921\u001b[39m, in \u001b[36mtrain_test_split\u001b[39m\u001b[34m(test_size, train_size, random_state, shuffle, stratify, *arrays)\u001b[39m\n\u001b[32m   2918\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m n_arrays == \u001b[32m0\u001b[39m:\n\u001b[32m   2919\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[33m\"\u001b[39m\u001b[33mAt least one array required as input\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m-> \u001b[39m\u001b[32m2921\u001b[39m arrays = \u001b[43mindexable\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43marrays\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2923\u001b[39m n_samples = _num_samples(arrays[\u001b[32m0\u001b[39m])\n\u001b[32m   2924\u001b[39m n_train, n_test = _validate_shuffle_split(\n\u001b[32m   2925\u001b[39m     n_samples, test_size, train_size, default_test_size=\u001b[32m0.25\u001b[39m\n\u001b[32m   2926\u001b[39m )\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\jaspe\\Documents\\Github_projects\\DSAIT4020-A2\\venv\\Lib\\site-packages\\sklearn\\utils\\validation.py:521\u001b[39m, in \u001b[36mindexable\u001b[39m\u001b[34m(*iterables)\u001b[39m\n\u001b[32m    491\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"Make arrays indexable for cross-validation.\u001b[39;00m\n\u001b[32m    492\u001b[39m \n\u001b[32m    493\u001b[39m \u001b[33;03mChecks consistent length, passes through None, and ensures that everything\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    517\u001b[39m \u001b[33;03m[[1, 2, 3], array([2, 3, 4]), None, <...Sparse...dtype 'int64'...shape (3, 1)>]\u001b[39;00m\n\u001b[32m    518\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    520\u001b[39m result = [_make_indexable(X) \u001b[38;5;28;01mfor\u001b[39;00m X \u001b[38;5;129;01min\u001b[39;00m iterables]\n\u001b[32m--> \u001b[39m\u001b[32m521\u001b[39m \u001b[43mcheck_consistent_length\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43mresult\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    522\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\jaspe\\Documents\\Github_projects\\DSAIT4020-A2\\venv\\Lib\\site-packages\\sklearn\\utils\\validation.py:464\u001b[39m, in \u001b[36mcheck_consistent_length\u001b[39m\u001b[34m(*arrays)\u001b[39m\n\u001b[32m    462\u001b[39m lengths = [_num_samples(X) \u001b[38;5;28;01mfor\u001b[39;00m X \u001b[38;5;129;01min\u001b[39;00m arrays \u001b[38;5;28;01mif\u001b[39;00m X \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m]\n\u001b[32m    463\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mset\u001b[39m(lengths)) > \u001b[32m1\u001b[39m:\n\u001b[32m--> \u001b[39m\u001b[32m464\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m    465\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mFound input variables with inconsistent numbers of samples: \u001b[39m\u001b[38;5;132;01m%r\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m    466\u001b[39m         % [\u001b[38;5;28mint\u001b[39m(l) \u001b[38;5;28;01mfor\u001b[39;00m l \u001b[38;5;129;01min\u001b[39;00m lengths]\n\u001b[32m    467\u001b[39m     )\n",
      "\u001b[31mValueError\u001b[39m: Found input variables with inconsistent numbers of samples: [10, 5000]"
     ]
    }
   ],
   "source": [
    "from preprocessing_depression import clean_and_preprocess_depression_data\n",
    "from preprocessing_insomnia import clean_and_preprocess_insomnia_data\n",
    "from preprocessing_electrical_circuit import gen_and_preprocess_ec_data\n",
    "\n",
    "dataset = pd.read_csv(TARGET_FILE_PATH + '/depression_data.csv')\n",
    "\n",
    "dep_X_train, dep_X_test, dep_y_train, dep_y_test, dep_y_embed_train, dep_y_embed_test = clean_and_preprocess_depression_data(dataset, RAW_DATA_FOLDER, TEST_SET_FRACTION, STATE, MISSING_VALUES_THRESHOLD)\n",
    "ins_X_train, ins_X_test, ins_y_train, ins_y_test, ins_y_embed_train, ins_y_embed_test = \n",
    "ec_X_train, ec_X_test, ec_y_train, ec_y_test, ec_y_embed_train, ec_y_embed_test = gen_and_preprocess_ec_data(SAMPLES_ELECTRICAL_CIRCUIT, TEST_SET_FRACTION, STATE)\n",
    "\n",
    "#TODO Fix issue with the time columns SLQ300/310/320/330"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "907be545",
   "metadata": {},
   "source": [
    "# **Models**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3383a4b1",
   "metadata": {},
   "source": [
    "### Training Bayesian Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "639c3e12",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      " ######################################## Binary GaussianNB Model ########################################\n",
      "F1 score: 0.2334384858044164\n",
      "ROC-AUC: 0.728135593220339\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.96      0.71      0.82       775\n",
      "           1       0.14      0.63      0.23        59\n",
      "\n",
      "    accuracy                           0.71       834\n",
      "   macro avg       0.55      0.67      0.53       834\n",
      "weighted avg       0.90      0.71      0.78       834\n",
      "\n",
      "Confusion matrix:\n",
      " [[554 221]\n",
      " [ 22  37]]\n",
      "\n",
      "\n",
      " ######################################## Multitarget Bayesian Ridge Model ########################################\n",
      "MSE per DPQ item: [0.6081899  0.55723907 0.83870486 0.6835447  0.69565646 0.62180295\n",
      " 0.64496949 0.41919515 0.20707469]\n",
      "Average MSE: 0.5862641415149625\n",
      "F1 score: 0.3157894736842105\n",
      "ROC-AUC: 0.5984691088026243\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.94      0.99      0.97       775\n",
      "           1       0.71      0.20      0.32        59\n",
      "\n",
      "    accuracy                           0.94       834\n",
      "   macro avg       0.82      0.60      0.64       834\n",
      "weighted avg       0.93      0.94      0.92       834\n",
      "\n",
      "Confusion matrix:\n",
      " [[770   5]\n",
      " [ 47  12]]\n",
      "\n",
      "\n",
      " ######################################## GaussianNB Model with Embedding Features ########################################\n",
      "F1 score: 0.452991452991453\n",
      "ROC-AUC: 0.9127063969382175\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      0.84      0.91       775\n",
      "           1       0.30      0.90      0.45        59\n",
      "\n",
      "    accuracy                           0.85       834\n",
      "   macro avg       0.65      0.87      0.68       834\n",
      "weighted avg       0.94      0.85      0.88       834\n",
      "\n",
      "Confusion matrix:\n",
      " [[653 122]\n",
      " [  6  53]]\n"
     ]
    }
   ],
   "source": [
    "from bayesian_models import *\n",
    "\n",
    "train_binary_bayes(X_train, X_test, y_train, y_test)\n",
    "train_multitarget_bayes(X_train, X_test, y_train, y_test, y_embed_train, y_embed_test)\n",
    "train_bayes_with_embed(X_train, X_test, y_train, y_test, y_embed_train, y_embed_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90ac3cc7",
   "metadata": {},
   "source": [
    "### Training Random Forests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c4743c7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters\n",
    "N_ESTIMATORS = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "dbaeae85",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      " ######################################## binary Forest  ########################################\n",
      "F1 score: 0.03333333333333333\n",
      "ROC-AUC: 0.8023728813559323\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.93      1.00      0.96       775\n",
      "           1       1.00      0.02      0.03        59\n",
      "\n",
      "    accuracy                           0.93       834\n",
      "   macro avg       0.97      0.51      0.50       834\n",
      "weighted avg       0.94      0.93      0.90       834\n",
      "\n",
      "Confusion matrix:\n",
      " [[775   0]\n",
      " [ 58   1]]\n",
      "\n",
      "\n",
      " ######################################## Multitarget Forest  ########################################\n",
      "MSE per DPQ item: [0.66152182 0.58451823 0.85873981 0.69939856 0.7436259  0.65868909\n",
      " 0.67468957 0.45619149 0.2308952 ]\n",
      "Average MSE: 0.6186966293631762\n",
      "F1 score: 0.29213483146067415\n",
      "ROC-AUC: 0.5992017495899399\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.94      0.98      0.96       775\n",
      "           1       0.43      0.22      0.29        59\n",
      "\n",
      "    accuracy                           0.92       834\n",
      "   macro avg       0.69      0.60      0.63       834\n",
      "weighted avg       0.91      0.92      0.91       834\n",
      "\n",
      "Confusion matrix:\n",
      " [[758  17]\n",
      " [ 46  13]]\n",
      "\n",
      "\n",
      " ######################################## Forest with embedding as faetures  ########################################\n",
      "F1 score: 0.7708333333333334\n",
      "ROC-AUC: 0.9974302897758337\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.97      1.00      0.99       775\n",
      "           1       1.00      0.63      0.77        59\n",
      "\n",
      "    accuracy                           0.97       834\n",
      "   macro avg       0.99      0.81      0.88       834\n",
      "weighted avg       0.97      0.97      0.97       834\n",
      "\n",
      "Confusion matrix:\n",
      " [[775   0]\n",
      " [ 22  37]]\n"
     ]
    }
   ],
   "source": [
    "from forest_models import *\n",
    "\n",
    "train_binary_forest(N_ESTIMATORS, STATE, X_train, X_test, y_train, y_test)\n",
    "train_multitarget_forest(N_ESTIMATORS, STATE, X_train, X_test, y_train, y_test, y_embed_train, y_embed_test)\n",
    "train_forest_with_embed_input(N_ESTIMATORS, STATE, X_train, X_test, y_train, y_test, y_embed_train, y_embed_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3172fa83",
   "metadata": {},
   "source": [
    "### Training Logistic Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f84084e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters\n",
    "MAX_ITERATIONS = 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c3635769",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      " ######################################## Binary Logistic Regression  ########################################\n",
      "F1 score: 0.2918454935622318\n",
      "ROC-AUC: 0.7994532531437943\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.96      0.82      0.89       775\n",
      "           1       0.20      0.58      0.29        59\n",
      "\n",
      "    accuracy                           0.80       834\n",
      "   macro avg       0.58      0.70      0.59       834\n",
      "weighted avg       0.91      0.80      0.84       834\n",
      "\n",
      "Confusion matrix:\n",
      " [[635 140]\n",
      " [ 25  34]]\n",
      "\n",
      "\n",
      " ######################################## Multitarget Logistic Regression ########################################\n",
      "MSE per DPQ item: [2.43884892 1.30935252 1.80215827 1.51918465 1.65707434 1.35611511\n",
      " 1.47841727 2.08752998 1.5059952 ]\n",
      "Average MSE: 1.6838529176658674\n",
      "F1 score: 0.26523297491039427\n",
      "ROC-AUC: 0.6954948059048661\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.96      0.76      0.85       775\n",
      "           1       0.17      0.63      0.27        59\n",
      "\n",
      "    accuracy                           0.75       834\n",
      "   macro avg       0.57      0.70      0.56       834\n",
      "weighted avg       0.91      0.75      0.81       834\n",
      "\n",
      "Confusion matrix:\n",
      " [[592 183]\n",
      " [ 22  37]]\n",
      "\n",
      "\n",
      " ######################################## Logistic Regression with Embedding Features ########################################\n",
      "F1 score: 0.7194244604316546\n",
      "ROC-AUC: 0.9821104428649535\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      0.96      0.97       775\n",
      "           1       0.62      0.85      0.72        59\n",
      "\n",
      "    accuracy                           0.95       834\n",
      "   macro avg       0.81      0.90      0.85       834\n",
      "weighted avg       0.96      0.95      0.96       834\n",
      "\n",
      "Confusion matrix:\n",
      " [[745  30]\n",
      " [  9  50]]\n"
     ]
    }
   ],
   "source": [
    "from logistic_models import *\n",
    "\n",
    "train_binary_logistic(MAX_ITERATIONS, STATE, X_train, X_test, y_train, y_test)\n",
    "train_multitarget_logistic(MAX_ITERATIONS, STATE, X_train, X_test, y_train, y_test, y_embed_train, y_embed_test)\n",
    "train_logistic_with_embed_input(MAX_ITERATIONS, STATE, X_train, X_test, y_train, y_test, y_embed_train, y_embed_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "777438b4",
   "metadata": {},
   "source": [
    "### Train Split MLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "aebf6f8e",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'int' object is not callable",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mTypeError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[10]\u001b[39m\u001b[32m, line 5\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01msplit_model\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m train_split_model\n\u001b[32m      3\u001b[39m torch.device(\u001b[33m\"\u001b[39m\u001b[33mcuda\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m torch.cuda.is_available() \u001b[38;5;28;01melse\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mcpu\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m----> \u001b[39m\u001b[32m5\u001b[39m \u001b[43mtrain_split_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43m \u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX_test\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_test\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_embed_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_embed_test\u001b[49m\u001b[43m \u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\jaspe\\Documents\\Github_projects\\DSAIT4020-A2\\split_model.py:71\u001b[39m, in \u001b[36mtrain_split_model\u001b[39m\u001b[34m(X_train, X_test, y_train, y_test, y_embed_train, y_embed_test, device)\u001b[39m\n\u001b[32m     70\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mtrain_split_model\u001b[39m( X_train, X_test, y_train, y_test, y_embed_train, y_embed_test, device=\u001b[33m\"\u001b[39m\u001b[33mcpu\u001b[39m\u001b[33m\"\u001b[39m ):\n\u001b[32m---> \u001b[39m\u001b[32m71\u001b[39m \tn_features, n_samples = \u001b[43mX_train\u001b[49m\u001b[43m.\u001b[49m\u001b[43msize\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     72\u001b[39m \tembedding_size, _ = y_embed_train.size()\n\u001b[32m     74\u001b[39m \tmodel = SplitModel( n_features=n_features, embedding_size=embedding_size )\n",
      "\u001b[31mTypeError\u001b[39m: 'int' object is not callable"
     ]
    }
   ],
   "source": [
    "from split_model import train_split_model\n",
    "\n",
    "torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "train_split_model( X_train, X_test, y_train, y_test, y_embed_train, y_embed_test )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e3fbefb",
   "metadata": {},
   "source": [
    "### Train Joint MLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da297fb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from joint_model import train_joint_model\n",
    "\n",
    "train_joint_model( X_train, X_test, y_train, y_test, y_embed_train, y_embed_test )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv (3.12.10)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
